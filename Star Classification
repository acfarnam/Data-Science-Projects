# Classifying Stars Using a Neural Network

## Introduction

The dataset we'll be looking at contains information on stars. The goal is to classify the type of stars based on the other features provided. The features in the dataset include:

- Temperature
- Luminosity
- Radius
- Absolute magnitude
- Star type
- Star color 
- Spectral class

The categories of stars are as follows:

- Brown Dwarf = 0
- Red Dwarf = 1
- White Dwarf = 2
- Main Sequence = 3
- Supergiant = 4
- Hypergiant = 5

## Preparing the Data


```python
import pandas as pd

stars = pd.read_csv('Data Sets/Stars.csv')
```


```python
stars.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 240 entries, 0 to 239
    Data columns (total 7 columns):
    Temperature (K)           240 non-null int64
    Luminosity(L/Lo)          240 non-null float64
    Radius(R/Ro)              240 non-null float64
    Absolute magnitude(Mv)    240 non-null float64
    Star type                 240 non-null int64
    Star color                240 non-null object
    Spectral Class            240 non-null object
    dtypes: float64(3), int64(2), object(2)
    memory usage: 13.2+ KB


Luckily there are no missing data points; however, we do need to encode the categorical features. Let's take a look at the first five observations to get an idea of what we are working with.


```python
stars.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Temperature (K)</th>
      <th>Luminosity(L/Lo)</th>
      <th>Radius(R/Ro)</th>
      <th>Absolute magnitude(Mv)</th>
      <th>Star type</th>
      <th>Star color</th>
      <th>Spectral Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3068</td>
      <td>0.002400</td>
      <td>0.1700</td>
      <td>16.12</td>
      <td>0</td>
      <td>Red</td>
      <td>M</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3042</td>
      <td>0.000500</td>
      <td>0.1542</td>
      <td>16.60</td>
      <td>0</td>
      <td>Red</td>
      <td>M</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2600</td>
      <td>0.000300</td>
      <td>0.1020</td>
      <td>18.70</td>
      <td>0</td>
      <td>Red</td>
      <td>M</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2800</td>
      <td>0.000200</td>
      <td>0.1600</td>
      <td>16.65</td>
      <td>0</td>
      <td>Red</td>
      <td>M</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1939</td>
      <td>0.000138</td>
      <td>0.1030</td>
      <td>20.06</td>
      <td>0</td>
      <td>Red</td>
      <td>M</td>
    </tr>
  </tbody>
</table>
</div>



We'll need to encode both the star color and spectral class. Star type will also have to be encoded, but we will wait until we assign star type as the target.


```python
stars = pd.get_dummies(stars, columns=['Star color', 'Spectral Class'])
```


```python
stars.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Temperature (K)</th>
      <th>Luminosity(L/Lo)</th>
      <th>Radius(R/Ro)</th>
      <th>Absolute magnitude(Mv)</th>
      <th>Star type</th>
      <th>Star color_Blue</th>
      <th>Star color_Blue</th>
      <th>Star color_Blue White</th>
      <th>Star color_Blue white</th>
      <th>Star color_Blue white</th>
      <th>...</th>
      <th>Star color_white</th>
      <th>Star color_yellow-white</th>
      <th>Star color_yellowish</th>
      <th>Spectral Class_A</th>
      <th>Spectral Class_B</th>
      <th>Spectral Class_F</th>
      <th>Spectral Class_G</th>
      <th>Spectral Class_K</th>
      <th>Spectral Class_M</th>
      <th>Spectral Class_O</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3068</td>
      <td>0.002400</td>
      <td>0.1700</td>
      <td>16.12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3042</td>
      <td>0.000500</td>
      <td>0.1542</td>
      <td>16.60</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2600</td>
      <td>0.000300</td>
      <td>0.1020</td>
      <td>18.70</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2800</td>
      <td>0.000200</td>
      <td>0.1600</td>
      <td>16.65</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1939</td>
      <td>0.000138</td>
      <td>0.1030</td>
      <td>20.06</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 31 columns</p>
</div>



This more than doubles our dimensions. Luckily we didn't start with that many features.

### Splitting Data into Train and Test Sets

Let's go ahead and split the data into train and test sets. We will also encode the star type now that it has been dropped and made into the target dataset.


```python
from sklearn.model_selection import train_test_split

predictors = stars.drop('Star type', axis = 1)
target = stars['Star type']
target = pd.get_dummies(target)
X_train, X_test, Y_train, Y_test = train_test_split(predictors, target, 
                                                    test_size = 0.20, 
                                                    random_state = 1)
```

### Pre-processing

Next we will normalize the data using the standard scaler in sklearn. The data will have a mean of zero and a standard deviation of one.


```python
from sklearn import preprocessing

std_scaler = preprocessing.StandardScaler().fit(X_train)

def scaler(X):
    x_norm_arr = std_scaler.fit_transform(X)
    return pd.DataFrame(x_norm_arr, columns = X.columns, index = X.index)

X_train = scaler(X_train)
X_test = scaler(X_test)
```

### Defining the Model

Let's go ahead and build the model we are going to be training. We'll begin with four hidden layers, each activated with the rectified linear unit function. The final layer will have an output of six, representing the six possible star types. We'll calculate loss using categorical crossentropy and optimize with the ADAM optimizer. We would like to measure the success of the model by calculating the accuracy.


```python
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

def build_model():
    model = Sequential()
    model.add(Dense(128, activation = 'relu', 
                    input_dim = X_train.shape[1]))
    model.add(Dense(128, activation = 'relu'))
    model.add(Dense(64, activation = 'relu'))
    model.add(Dense(64, activation = 'relu'))
    model.add(Dense(32, activation = 'relu'))
    model.add(Dense(6, activation = 'softmax'))

    model.compile(loss = 'categorical_crossentropy', 
                  optimizer = 'adam', 
                  metrics = ['acc'])
    return model
```

    Using TensorFlow backend.


Due to the small sample size, we will use k-fold cross validation to observe the models performance during the training process. Let's use 5-fold cross validation.


```python
import numpy as np

k = 5
num_val_samples = len(X_train) // k
num_epochs = 100

all_loss_histories = []
all_val_loss_histories = []
all_acc_histories = []
all_val_acc_histories = []

for i in range(k):
    print('Processing Fold #', i + 1)
    
    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = Y_train[i * num_val_samples: (i + 1) * num_val_samples]

    partial_train_data = np.concatenate(
        [X_train[:i * num_val_samples],
         X_train[(i + 1) * num_val_samples:]],
        axis = 0)
    partial_train_targets = np.concatenate(
        [Y_train[:i * num_val_samples],
         Y_train[(i + 1) * num_val_samples:]],
        axis = 0)
    
    model = build_model()
    history = model.fit(partial_train_data, partial_train_targets,
                        epochs = num_epochs, 
                        batch_size = 1, 
                        verbose = 0,
                        validation_data=(val_data, val_targets))
    
    loss_history = history.history['loss']
    all_loss_histories.append(loss_history)
    val_loss_history = history.history['val_loss']
    all_val_loss_histories.append(val_loss_history)
    
    acc_history = history.history['acc']
    all_acc_histories.append(acc_history)
    val_acc_history = history.history['val_acc']
    all_val_acc_histories.append(val_acc_history)
```

    Processing Fold # 1
    WARNING:tensorflow:From /Users/cain/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.
    
    Processing Fold # 2
    Processing Fold # 3
    Processing Fold # 4
    Processing Fold # 5


### Plotting the Performance


```python
average_loss_history = [
    np.mean([x[i] for x in all_loss_histories]) for i in range(100)]

average_val_loss_history = [
    np.mean([x[i] for x in all_val_loss_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_loss_history) + 1), average_loss_history)
plt.plot(range(1, len(average_val_loss_history) + 1), average_val_loss_history)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc = 'upper left')
plt.savefig('stars_loss1.jpg')
plt.show()
```


    <Figure size 640x480 with 1 Axes>


The model is overfitting as quickly as 10 epochs. Let's see how the accuracy compares.


```python
average_acc_history = [
    np.mean([x[i] for x in all_acc_histories]) for i in range(100)]

average_val_acc_history = [
    np.mean([x[i] for x in all_val_acc_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_acc_history) + 1), average_acc_history)
plt.plot(range(1, len(average_val_acc_history) + 1), average_val_acc_history)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc = 'lower right')
plt.savefig('stars_acc1.jpg')
plt.show()
```


![png](output_28_0.png)


The training accuracy converges quickly. We can afford to reduce the size of the model without having to lose any prediction power.

Let's remove a few hidden levels and perfrom 5-fold validation again.


```python
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

def build_model():
    model = Sequential()
    model.add(Dense(128, activation = 'relu', 
                    input_dim = X_train.shape[1]))
    model.add(Dense(64, activation = 'relu'))
    model.add(Dense(32, activation = 'relu'))
    model.add(Dense(6, activation = 'softmax'))

    model.compile(loss = 'categorical_crossentropy', 
                  optimizer = 'adam', 
                  metrics = ['acc'])
    return model
```


```python
import numpy as np

k = 5
num_val_samples = len(X_train) // k
num_epochs = 100

all_loss_histories = []
all_val_loss_histories = []
all_acc_histories = []
all_val_acc_histories = []

for i in range(k):
    print('Processing Fold #', i + 1)
    
    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = Y_train[i * num_val_samples: (i + 1) * num_val_samples]

    partial_train_data = np.concatenate(
        [X_train[:i * num_val_samples],
         X_train[(i + 1) * num_val_samples:]],
        axis = 0)
    partial_train_targets = np.concatenate(
        [Y_train[:i * num_val_samples],
         Y_train[(i + 1) * num_val_samples:]],
        axis = 0)
    
    model = build_model()
    history = model.fit(partial_train_data, partial_train_targets,
                        epochs = num_epochs, 
                        batch_size = 1, 
                        verbose = 0,
                        validation_data=(val_data, val_targets))
    
    loss_history = history.history['loss']
    all_loss_histories.append(loss_history)
    val_loss_history = history.history['val_loss']
    all_val_loss_histories.append(val_loss_history)
    
    acc_history = history.history['acc']
    all_acc_histories.append(acc_history)
    val_acc_history = history.history['val_acc']
    all_val_acc_histories.append(val_acc_history)
```

    Processing Fold # 1
    Processing Fold # 2
    Processing Fold # 3
    Processing Fold # 4
    Processing Fold # 5



```python
average_loss_history = [
    np.mean([x[i] for x in all_loss_histories]) for i in range(100)]

average_val_loss_history = [
    np.mean([x[i] for x in all_val_loss_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_loss_history) + 1), average_loss_history)
plt.plot(range(1, len(average_val_loss_history) + 1), average_val_loss_history)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc = 'upper left')
plt.savefig('stars_loss2.jpg')
plt.show()
```


![png](output_34_0.png)


It's definitely an improvement, but we can do better. Let's check the accuracy before we decide how to proceed.


```python
average_acc_history = [
    np.mean([x[i] for x in all_acc_histories]) for i in range(100)]

average_val_acc_history = [
    np.mean([x[i] for x in all_val_acc_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_acc_history) + 1), average_acc_history)
plt.plot(range(1, len(average_val_acc_history) + 1), average_val_acc_history)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc = 'lower right')
plt.savefig('stars_acc2.jpg')
plt.show()
```


![png](output_37_0.png)


We still have plenty of prediction power. Let's reduce the size of the layers again.


```python
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

def build_model():
    model = Sequential()
    model.add(Dense(64, activation = 'relu', 
                    input_dim = X_train.shape[1]))
    model.add(Dense(32, activation = 'relu'))
    model.add(Dense(16, activation = 'relu'))
    model.add(Dense(6, activation = 'softmax'))

    model.compile(loss = 'categorical_crossentropy', 
                  optimizer = 'adam', 
                  metrics = ['acc'])
    return model
```


```python
import numpy as np

k = 5
num_val_samples = len(X_train) // k
num_epochs = 100

all_loss_histories = []
all_val_loss_histories = []
all_acc_histories = []
all_val_acc_histories = []

for i in range(k):
    print('Processing Fold #', i + 1)
    
    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = Y_train[i * num_val_samples: (i + 1) * num_val_samples]

    partial_train_data = np.concatenate(
        [X_train[:i * num_val_samples],
         X_train[(i + 1) * num_val_samples:]],
        axis = 0)
    partial_train_targets = np.concatenate(
        [Y_train[:i * num_val_samples],
         Y_train[(i + 1) * num_val_samples:]],
        axis = 0)
    
    model = build_model()
    history = model.fit(partial_train_data, partial_train_targets,
                        epochs = num_epochs, 
                        batch_size = 1, 
                        verbose = 0,
                        validation_data=(val_data, val_targets))
    
    loss_history = history.history['loss']
    all_loss_histories.append(loss_history)
    val_loss_history = history.history['val_loss']
    all_val_loss_histories.append(val_loss_history)
    
    acc_history = history.history['acc']
    all_acc_histories.append(acc_history)
    val_acc_history = history.history['val_acc']
    all_val_acc_histories.append(val_acc_history)
```

    Processing Fold # 1
    Processing Fold # 2
    Processing Fold # 3
    Processing Fold # 4
    Processing Fold # 5



```python
average_loss_history = [
    np.mean([x[i] for x in all_loss_histories]) for i in range(100)]

average_val_loss_history = [
    np.mean([x[i] for x in all_val_loss_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_loss_history) + 1), average_loss_history)
plt.plot(range(1, len(average_val_loss_history) + 1), average_val_loss_history)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc = 'upper left')
plt.savefig('stars_loss3.jpg')
plt.show()
```


![png](output_42_0.png)



```python
average_acc_history = [
    np.mean([x[i] for x in all_acc_histories]) for i in range(100)]

average_val_acc_history = [
    np.mean([x[i] for x in all_val_acc_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_acc_history) + 1), average_acc_history)
plt.plot(range(1, len(average_val_acc_history) + 1), average_val_acc_history)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc = 'lower right')
plt.savefig('stars_acc3.jpg')
plt.show()
```


![png](output_44_0.png)



```python
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.layers import Dropout

def build_model():
    model = Sequential()
    model.add(Dense(64, activation = 'relu', 
                    input_dim = X_train.shape[1]))
    model.add(Dropout(.2))
    model.add(Dense(32, activation = 'relu'))
    model.add(Dropout(.2))
    model.add(Dense(16, activation = 'relu'))
    model.add(Dropout(.2))
    model.add(Dense(6, activation = 'softmax'))

    model.compile(loss = 'categorical_crossentropy', 
                  optimizer = 'adam', 
                  metrics = ['acc'])
    return model
```


```python
import numpy as np

k = 5
num_val_samples = len(X_train) // k
num_epochs = 100

all_loss_histories = []
all_val_loss_histories = []
all_acc_histories = []
all_val_acc_histories = []

for i in range(k):
    print('Processing Fold #', i + 1)
    
    val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = Y_train[i * num_val_samples: (i + 1) * num_val_samples]

    partial_train_data = np.concatenate(
        [X_train[:i * num_val_samples],
         X_train[(i + 1) * num_val_samples:]],
        axis = 0)
    partial_train_targets = np.concatenate(
        [Y_train[:i * num_val_samples],
         Y_train[(i + 1) * num_val_samples:]],
        axis = 0)
    
    model = build_model()
    history = model.fit(partial_train_data, partial_train_targets,
                        epochs = num_epochs, 
                        batch_size = 1, 
                        verbose = 0,
                        validation_data=(val_data, val_targets))
    
    loss_history = history.history['loss']
    all_loss_histories.append(loss_history)
    val_loss_history = history.history['val_loss']
    all_val_loss_histories.append(val_loss_history)
    
    acc_history = history.history['acc']
    all_acc_histories.append(acc_history)
    val_acc_history = history.history['val_acc']
    all_val_acc_histories.append(val_acc_history)
```

    Processing Fold # 1
    Processing Fold # 2
    Processing Fold # 3
    Processing Fold # 4
    Processing Fold # 5



```python
average_loss_history = [
    np.mean([x[i] for x in all_loss_histories]) for i in range(100)]

average_val_loss_history = [
    np.mean([x[i] for x in all_val_loss_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_loss_history) + 1), average_loss_history)
plt.plot(range(1, len(average_val_loss_history) + 1), average_val_loss_history)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc = 'upper left')
plt.savefig('stars_loss4.jpg')
plt.show()
```


![png](output_48_0.png)



```python
average_acc_history = [
    np.mean([x[i] for x in all_acc_histories]) for i in range(100)]

average_val_acc_history = [
    np.mean([x[i] for x in all_val_acc_histories]) for i in range(100)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_acc_history) + 1), average_acc_history)
plt.plot(range(1, len(average_val_acc_history) + 1), average_val_acc_history)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc = 'lower right')
plt.savefig('stars_acc4.jpg')
plt.show()
```


![png](output_50_0.png)



```python
model = build_model()

history = model.fit(X_train, Y_train, 
                    epochs = 40, 
                    batch_size = 1)
```

    Epoch 1/40
    192/192 [==============================] - 1s 7ms/step - loss: 1.6636 - acc: 0.3698
    Epoch 2/40
    192/192 [==============================] - 0s 2ms/step - loss: 1.1419 - acc: 0.5885
    Epoch 3/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.9428 - acc: 0.6354
    Epoch 4/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.7171 - acc: 0.6927
    Epoch 5/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.5740 - acc: 0.7448
    Epoch 6/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.5100 - acc: 0.7708
    Epoch 7/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.4882 - acc: 0.7760
    Epoch 8/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.4530 - acc: 0.7708
    Epoch 9/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.4089 - acc: 0.8229
    Epoch 10/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.4416 - acc: 0.7969
    Epoch 11/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.3935 - acc: 0.8385
    Epoch 12/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.3913 - acc: 0.8490
    Epoch 13/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.3221 - acc: 0.8490
    Epoch 14/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.2453 - acc: 0.9010
    Epoch 15/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.3472 - acc: 0.8542
    Epoch 16/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.2957 - acc: 0.8906
    Epoch 17/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.2718 - acc: 0.8802
    Epoch 18/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1918 - acc: 0.9427
    Epoch 19/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.2811 - acc: 0.8906
    Epoch 20/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1881 - acc: 0.9115
    Epoch 21/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.2556 - acc: 0.8958
    Epoch 22/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1850 - acc: 0.9479
    Epoch 23/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.2500 - acc: 0.8854
    Epoch 24/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1789 - acc: 0.9167
    Epoch 25/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.2213 - acc: 0.8906
    Epoch 26/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1980 - acc: 0.9479
    Epoch 27/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1324 - acc: 0.9479
    Epoch 28/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1596 - acc: 0.9427
    Epoch 29/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1133 - acc: 0.9635
    Epoch 30/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1883 - acc: 0.9323
    Epoch 31/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0708 - acc: 0.9844
    Epoch 32/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0917 - acc: 0.9531
    Epoch 33/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1650 - acc: 0.9531
    Epoch 34/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0740 - acc: 0.9844
    Epoch 35/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0677 - acc: 0.9792
    Epoch 36/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.1025 - acc: 0.9531
    Epoch 37/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0681 - acc: 0.9688
    Epoch 38/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0960 - acc: 0.9688
    Epoch 39/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0769 - acc: 0.9792
    Epoch 40/40
    192/192 [==============================] - 0s 2ms/step - loss: 0.0489 - acc: 0.9740



```python
loss, acc = model.evaluate(X_test, Y_test)
print('The accuracy for this model is: ', round(acc, 3))
```

    48/48 [==============================] - 0s 7ms/step
    The accuracy for this model is:  1.0

